{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColBERTv2: Indexing & Search Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the relevant classes. As we'll see below, `Indexer` and `Searcher` are the key actors here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Indexer, Searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow here assumes an IR dataset: a set of queries and a corresponding collection of passages.\n",
    "\n",
    "The classes `Queries` and `Collection` provide a convenient interface for working with such datasets.\n",
    "\n",
    "We will use the *dev set* of the **LoTTE benchmark** we recently introduced in the ColBERTv2 paper. The dev and test sets contain several domain-specific corpora, and we'll use the smallest dev set corpus, namely `lifestyle:dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-12 15:54:09--  https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz\n",
      "正在解析主机 downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "正在连接 downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度： 405924985 (387M) [application/octet-stream]\n",
      "正在保存至: ‘downloads/colbertv2.0.tar.gz’\n",
      "\n",
      "colbertv2.0.tar.gz  100%[===================>] 387.12M  5.02MB/s    用时 76s     \n",
      "\n",
      "2023-02-12 15:55:26 (5.12 MB/s) - 已保存 ‘downloads/colbertv2.0.tar.gz’ [405924985/405924985])\n",
      "\n",
      "colbertv2.0/\n",
      "colbertv2.0/artifact.metadata\n",
      "colbertv2.0/vocab.txt\n",
      "colbertv2.0/tokenizer.json\n",
      "colbertv2.0/special_tokens_map.json\n",
      "colbertv2.0/tokenizer_config.json\n",
      "colbertv2.0/config.json\n",
      "colbertv2.0/pytorch_model.bin\n",
      "--2023-02-12 15:55:28--  https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/lotte.tar.gz\n",
      "正在解析主机 downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "正在连接 downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度： 3576167599 (3.3G) [application/octet-stream]\n",
      "正在保存至: ‘downloads/lotte.tar.gz’\n",
      "\n",
      "lotte.tar.gz        100%[===================>]   3.33G  4.99MB/s    用时 11m 17s \n",
      "\n",
      "2023-02-12 16:06:46 (5.04 MB/s) - 已保存 ‘downloads/lotte.tar.gz’ [3576167599/3576167599])\n",
      "\n",
      "lotte/\n",
      "lotte/science/\n",
      "lotte/science/test/\n",
      "lotte/science/test/questions.search.tsv\n",
      "lotte/science/test/questions.forum.tsv\n",
      "lotte/science/test/collection.tsv\n",
      "lotte/science/test/qas.forum.jsonl\n",
      "lotte/science/test/metadata.jsonl\n",
      "lotte/science/test/qas.search.jsonl\n",
      "lotte/science/dev/\n",
      "lotte/science/dev/questions.search.tsv\n",
      "lotte/science/dev/questions.forum.tsv\n",
      "lotte/science/dev/collection.tsv\n",
      "lotte/science/dev/qas.forum.jsonl\n",
      "lotte/science/dev/metadata.jsonl\n",
      "lotte/science/dev/qas.search.jsonl\n",
      "lotte/writing/\n",
      "lotte/writing/test/\n",
      "lotte/writing/test/questions.search.tsv\n",
      "lotte/writing/test/questions.forum.tsv\n",
      "lotte/writing/test/collection.tsv\n",
      "lotte/writing/test/qas.forum.jsonl\n",
      "lotte/writing/test/metadata.jsonl\n",
      "lotte/writing/test/qas.search.jsonl\n",
      "lotte/writing/dev/\n",
      "lotte/writing/dev/questions.search.tsv\n",
      "lotte/writing/dev/questions.forum.tsv\n",
      "lotte/writing/dev/collection.tsv\n",
      "lotte/writing/dev/qas.forum.jsonl\n",
      "lotte/writing/dev/metadata.jsonl\n",
      "lotte/writing/dev/qas.search.jsonl\n",
      "lotte/recreation/\n",
      "lotte/recreation/test/\n",
      "lotte/recreation/test/questions.search.tsv\n",
      "lotte/recreation/test/questions.forum.tsv\n",
      "lotte/recreation/test/collection.tsv\n",
      "lotte/recreation/test/qas.forum.jsonl\n",
      "lotte/recreation/test/metadata.jsonl\n",
      "lotte/recreation/test/qas.search.jsonl\n",
      "lotte/recreation/dev/\n",
      "lotte/recreation/dev/questions.search.tsv\n",
      "lotte/recreation/dev/questions.forum.tsv\n",
      "lotte/recreation/dev/collection.tsv\n",
      "lotte/recreation/dev/qas.forum.jsonl\n",
      "lotte/recreation/dev/metadata.jsonl\n",
      "lotte/recreation/dev/qas.search.jsonl\n",
      "lotte/lifestyle/\n",
      "lotte/lifestyle/test/\n",
      "lotte/lifestyle/test/questions.search.tsv\n",
      "lotte/lifestyle/test/questions.forum.tsv\n",
      "lotte/lifestyle/test/collection.tsv\n",
      "lotte/lifestyle/test/qas.forum.jsonl\n",
      "lotte/lifestyle/test/metadata.jsonl\n",
      "lotte/lifestyle/test/qas.search.jsonl\n",
      "lotte/lifestyle/dev/\n",
      "lotte/lifestyle/dev/questions.search.tsv\n",
      "lotte/lifestyle/dev/questions.forum.tsv\n",
      "lotte/lifestyle/dev/collection.tsv\n",
      "lotte/lifestyle/dev/qas.forum.jsonl\n",
      "lotte/lifestyle/dev/metadata.jsonl\n",
      "lotte/lifestyle/dev/qas.search.jsonl\n",
      "lotte/evaluate_lotte_rankings.py\n",
      "lotte/technology/\n",
      "lotte/technology/test/\n",
      "lotte/technology/test/questions.search.tsv\n",
      "lotte/technology/test/questions.forum.tsv\n",
      "lotte/technology/test/collection.tsv\n",
      "lotte/technology/test/qas.forum.jsonl\n",
      "lotte/technology/test/metadata.jsonl\n",
      "lotte/technology/test/qas.search.jsonl\n",
      "lotte/technology/dev/\n",
      "lotte/technology/dev/questions.search.tsv\n",
      "lotte/technology/dev/questions.forum.tsv\n",
      "lotte/technology/dev/collection.tsv\n",
      "lotte/technology/dev/qas.forum.jsonl\n",
      "lotte/technology/dev/metadata.jsonl\n",
      "lotte/technology/dev/qas.search.jsonl\n",
      "lotte/pooled/\n",
      "lotte/pooled/test/\n",
      "lotte/pooled/test/questions.search.tsv\n",
      "lotte/pooled/test/questions.forum.tsv\n",
      "lotte/pooled/test/collection.tsv\n",
      "lotte/pooled/test/qas.forum.jsonl\n",
      "lotte/pooled/test/qas.search.jsonl\n",
      "lotte/pooled/dev/\n",
      "lotte/pooled/dev/questions.search.tsv\n",
      "lotte/pooled/dev/questions.forum.tsv\n",
      "lotte/pooled/dev/collection.tsv\n",
      "lotte/pooled/dev/qas.forum.jsonl\n",
      "lotte/pooled/dev/qas.search.jsonl\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p downloads/\n",
    "\n",
    "# ColBERTv2 checkpoint trained on MS MARCO Passage Ranking (388MB compressed)\n",
    "!wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -P downloads/\n",
    "!tar -xvzf downloads/colbertv2.0.tar.gz -C downloads/\n",
    "\n",
    "# The LoTTE dev and test sets (3.4GB compressed)\n",
    "!wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/lotte.tar.gz -P downloads/\n",
    "!tar -xvzf downloads/lotte.tar.gz -C downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 04:42:29] #> Loading the queries from docs/downloads/lotte/lifestyle/dev/questions.search.tsv ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'docs/downloads/lotte/lifestyle/dev/questions.search.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m queries \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataroot, dataset, datasplit, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions.search.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m collection \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataroot, dataset, datasplit, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollection.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[43mQueries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m collection \u001b[38;5;241m=\u001b[39m Collection(path\u001b[38;5;241m=\u001b[39mcollection)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(queries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m queries and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(collection)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m passages\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/documents/ColBERT/docs/../colbert/data/queries.py:17\u001b[0m, in \u001b[0;36mQueries.__init__\u001b[0;34m(self, path, data)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;28mtype\u001b[39m(data)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_data(data) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/documents/ColBERT/docs/../colbert/data/queries.py:52\u001b[0m, in \u001b[0;36mQueries._load_file\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mload_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Load QAs\u001b[39;00m\n",
      "File \u001b[0;32m~/documents/ColBERT/docs/../colbert/evaluation/loaders.py:20\u001b[0m, in \u001b[0;36mload_queries\u001b[0;34m(queries_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m queries \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m     18\u001b[0m print_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#> Loading the queries from\u001b[39m\u001b[38;5;124m\"\u001b[39m, queries_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqueries_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     22\u001b[0m         qid, query, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docs/downloads/lotte/lifestyle/dev/questions.search.tsv'"
     ]
    }
   ],
   "source": [
    "dataroot = 'docs/downloads/lotte'\n",
    "dataset = 'lifestyle'\n",
    "datasplit = 'dev'\n",
    "\n",
    "queries = os.path.join(dataroot, dataset, datasplit, 'questions.search.tsv')\n",
    "collection = os.path.join(dataroot, dataset, datasplit, 'collection.tsv')\n",
    "\n",
    "queries = Queries(path=queries)\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(queries)} queries and {len(collection):,} passages'\n",
    "\n",
    "print(\"11\")\n",
    "print(collection[1145])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loaded 417 queries and 269k passages. Let's inspect one query and one passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are blossom end rot tomatoes edible?\n",
      "\n",
      "\"Good Harbor. Anita Diamant's international bestseller \"\"The Red Tent\"\" brilliantly re-created the ancient world of womanhood. Diamant brings her remarkable storytelling skills to \"\"Good Harbor\"\" -- offering insight to the precarious balance of marriage and career, motherhood and friendship in the world of modern women. The seaside town of Gloucester, Massachusetts is a place where the smell of the ocean lingers in the air and the rocky coast glistens in the Atlantic sunshine. When longtime Gloucester-resident Kathleen Levine is diagnosed with breast cancer, her life is thrown into turmoil. Frightened and burdened by secrets, she meets Joyce Tabachnik -- a freelance writer with literary aspirations -- and a once-in-a-lifetime friendship is born. Joyce has just bought a small house in Gloucester, where she hopes to write as well as vacation with her family. Like Kathleen, Joyce is at a fragile place in her life. A mutual love for books, humor, and the beauty of the natural world brings the two women together. They share their personal histories, and help each other to confront scars left by old emotional wounds. With her own trademark wisdom and humor, Diamant considers the nature, strength, and necessity of adult female friendship. \"\"Good Harbor\"\" examines the tragedy of loss, the insidious nature of family secrets, as well as the redemptive power of friendship.. \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(queries[24])\n",
    "print()\n",
    "print(collection[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "For efficient search, we can pre-compute the ColBERT representation of each passage and index them.\n",
    "\n",
    "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index.\n",
    "\n",
    "(With four Titan V GPUs, indexing should take about 13 minutes. The output is fairly long/ugly at the moment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300   # truncate passages at 300 tokens\n",
    "\n",
    "checkpoint = 'downloads/colbertv2.0'\n",
    "index_name = f'{dataset}.{datasplit}.{nbits}bits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Feb 23, 01:13:42] #> Note: Output directory /home/adqaicp/documents/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits already exists\n",
      "\n",
      "\n",
      "[Feb 23, 01:13:42] #> Will delete 50 files already at /home/adqaicp/documents/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits in 20 seconds...\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 1e-5,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 20000,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 64,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": \"bert-base-uncased\",\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"downloads\\/colbertv2.0\",\n",
      "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
      "    \"collection\": {\n",
      "        \"provenance\": \"downloads\\/lotte\\/lifestyle\\/dev\\/collection.tsv\"\n",
      "    },\n",
      "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
      "    \"index_name\": \"lifestyle.dev.2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/adqaicp\\/documents\\/ColBERT\\/docs\\/experiments\",\n",
      "    \"experiment\": \"notebook\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2023-02\\/22\\/23.36.59\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1\n",
      "}\n",
      "[Feb 23, 01:14:12] [0] \t\t # of sampled PIDs = 269295 \t sampled_pids[:3] = [1747430, 42652, 1252586]\n",
      "[Feb 23, 01:14:13] [0] \t\t #> Encoding 269295 passages..\n",
      "[Feb 23, 01:23:23] [0] \t\t avg_doclen_est = 140.2255401611328 \t len(local_sample) = 269,295\n",
      "[Feb 23, 01:23:31] [0] \t\t Creaing 262,144 partitions.\n",
      "[Feb 23, 01:23:31] [0] \t\t *Estimated* 331,024,122 embeddings.\n",
      "[Feb 23, 01:23:31] [0] \t\t #> Saving the indexing plan to /home/adqaicp/documents/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits/plan.json ..\n",
      "Clustering 37712037 points in 128D to 262144 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 2.25 s\n",
      "  Iteration 19 (4126.54 s, search 4089.65 s): objective=5.93446e+06 imbalance=1.291 nsplit=0       \n",
      "[Feb 23, 02:32:31] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Feb 23, 02:32:32] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.033, 0.031, 0.033, 0.029, 0.03, 0.031, 0.03, 0.029, 0.029, 0.031, 0.029, 0.031, 0.029, 0.032, 0.031, 0.031, 0.028, 0.029, 0.03, 0.029, 0.031, 0.031, 0.03, 0.031, 0.031, 0.029, 0.032, 0.03, 0.031, 0.031, 0.031, 0.033, 0.032, 0.029, 0.03, 0.028, 0.03, 0.029, 0.03, 0.034, 0.031, 0.033, 0.03, 0.029, 0.031, 0.029, 0.029, 0.033, 0.031, 0.029, 0.029, 0.029, 0.031, 0.031, 0.029, 0.03, 0.033, 0.031, 0.034, 0.029, 0.029, 0.031, 0.031, 0.032, 0.032, 0.032, 0.032, 0.031, 0.029, 0.029, 0.033, 0.031, 0.03, 0.031, 0.029, 0.03, 0.032, 0.033, 0.03, 0.032, 0.033, 0.03, 0.031, 0.031, 0.029, 0.03, 0.032, 0.03, 0.029, 0.036, 0.031, 0.032, 0.031, 0.032, 0.03, 0.031, 0.032, 0.029, 0.031, 0.031, 0.032, 0.031, 0.03, 0.032, 0.033, 0.03, 0.031, 0.029, 0.031, 0.029, 0.032, 0.03, 0.032, 0.029, 0.031, 0.031, 0.03, 0.031, 0.032, 0.031, 0.029, 0.03, 0.03, 0.032, 0.029, 0.03, 0.03, 0.029]\n",
      "[Feb 23, 02:32:33] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Feb 23, 02:32:33] #> Got bucket_cutoffs = tensor([-0.0245,  0.0000,  0.0246], device='cuda:0') and bucket_weights = tensor([-0.0433, -0.0114,  0.0114,  0.0435], device='cuda:0')\n",
      "[Feb 23, 02:32:33] avg_residual = 0.030670166015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:32:33] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:33:23] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 3,531,874 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:57, 57.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:33:30] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:34:20] [0] \t\t #> Saving chunk 1: \t 25,000 passages and 3,511,047 embeddings. From #25,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [01:53, 56.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:34:27] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:35:17] [0] \t\t #> Saving chunk 2: \t 25,000 passages and 3,519,290 embeddings. From #50,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [02:50, 56.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:35:24] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:36:14] [0] \t\t #> Saving chunk 3: \t 25,000 passages and 3,502,249 embeddings. From #75,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [03:47, 56.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:36:21] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:37:11] [0] \t\t #> Saving chunk 4: \t 25,000 passages and 3,488,977 embeddings. From #100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [04:44, 56.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:37:18] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:38:08] [0] \t\t #> Saving chunk 5: \t 25,000 passages and 3,520,990 embeddings. From #125,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [05:41, 56.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:38:15] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:39:05] [0] \t\t #> Saving chunk 6: \t 25,000 passages and 3,519,026 embeddings. From #150,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [06:38, 56.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:39:12] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:40:02] [0] \t\t #> Saving chunk 7: \t 25,000 passages and 3,489,306 embeddings. From #175,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [07:35, 56.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:40:09] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:40:59] [0] \t\t #> Saving chunk 8: \t 25,000 passages and 3,526,326 embeddings. From #200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [08:32, 56.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:41:06] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:41:56] [0] \t\t #> Saving chunk 9: \t 25,000 passages and 3,511,564 embeddings. From #225,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [09:29, 56.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:42:02] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:42:53] [0] \t\t #> Saving chunk 10: \t 25,000 passages and 3,520,353 embeddings. From #250,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [10:25, 56.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:42:59] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:43:50] [0] \t\t #> Saving chunk 11: \t 25,000 passages and 3,496,385 embeddings. From #275,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [11:23, 56.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:43:56] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:44:47] [0] \t\t #> Saving chunk 12: \t 25,000 passages and 3,542,408 embeddings. From #300,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [12:20, 57.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:44:54] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:45:44] [0] \t\t #> Saving chunk 13: \t 25,000 passages and 3,512,796 embeddings. From #325,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [13:17, 57.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:45:51] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:46:42] [0] \t\t #> Saving chunk 14: \t 25,000 passages and 3,530,088 embeddings. From #350,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [14:15, 57.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:46:49] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:47:39] [0] \t\t #> Saving chunk 15: \t 25,000 passages and 3,480,406 embeddings. From #375,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [15:12, 57.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:47:46] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:48:36] [0] \t\t #> Saving chunk 16: \t 25,000 passages and 3,492,325 embeddings. From #400,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [16:09, 57.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:48:43] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:49:33] [0] \t\t #> Saving chunk 17: \t 25,000 passages and 3,490,779 embeddings. From #425,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [17:06, 57.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:49:40] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:50:30] [0] \t\t #> Saving chunk 18: \t 25,000 passages and 3,527,422 embeddings. From #450,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [18:03, 57.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:50:37] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:51:27] [0] \t\t #> Saving chunk 19: \t 25,000 passages and 3,496,300 embeddings. From #475,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [19:00, 57.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:51:34] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:52:24] [0] \t\t #> Saving chunk 20: \t 25,000 passages and 3,517,603 embeddings. From #500,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [19:57, 57.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:52:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:53:21] [0] \t\t #> Saving chunk 21: \t 25,000 passages and 3,505,067 embeddings. From #525,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [20:54, 56.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:53:28] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:54:18] [0] \t\t #> Saving chunk 22: \t 25,000 passages and 3,484,407 embeddings. From #550,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [21:51, 56.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:54:25] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:55:15] [0] \t\t #> Saving chunk 23: \t 25,000 passages and 3,520,017 embeddings. From #575,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [22:48, 56.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:55:22] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:56:12] [0] \t\t #> Saving chunk 24: \t 25,000 passages and 3,495,823 embeddings. From #600,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "25it [23:45, 57.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:56:19] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:57:09] [0] \t\t #> Saving chunk 25: \t 25,000 passages and 3,485,067 embeddings. From #625,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "26it [24:42, 57.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:57:16] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:58:07] [0] \t\t #> Saving chunk 26: \t 25,000 passages and 3,491,009 embeddings. From #650,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "27it [25:40, 57.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:58:14] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 02:59:04] [0] \t\t #> Saving chunk 27: \t 25,000 passages and 3,486,524 embeddings. From #675,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "28it [26:37, 57.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 02:59:11] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:00:02] [0] \t\t #> Saving chunk 28: \t 25,000 passages and 3,502,630 embeddings. From #700,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "29it [27:35, 57.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:00:09] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:01:00] [0] \t\t #> Saving chunk 29: \t 25,000 passages and 3,478,553 embeddings. From #725,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "30it [28:33, 57.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:01:07] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:01:58] [0] \t\t #> Saving chunk 30: \t 25,000 passages and 3,501,648 embeddings. From #750,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "31it [29:31, 57.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:02:05] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:02:56] [0] \t\t #> Saving chunk 31: \t 25,000 passages and 3,513,308 embeddings. From #775,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "32it [30:29, 57.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:03:03] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:03:55] [0] \t\t #> Saving chunk 32: \t 25,000 passages and 3,472,200 embeddings. From #800,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "33it [31:28, 58.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:04:01] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:04:53] [0] \t\t #> Saving chunk 33: \t 25,000 passages and 3,524,446 embeddings. From #825,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [32:26, 58.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:05:00] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:05:50] [0] \t\t #> Saving chunk 34: \t 25,000 passages and 3,519,765 embeddings. From #850,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "35it [33:23, 57.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:05:57] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:06:49] [0] \t\t #> Saving chunk 35: \t 25,000 passages and 3,526,466 embeddings. From #875,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [34:22, 58.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:06:56] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:07:47] [0] \t\t #> Saving chunk 36: \t 25,000 passages and 3,496,219 embeddings. From #900,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "37it [35:20, 58.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:07:54] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:08:45] [0] \t\t #> Saving chunk 37: \t 25,000 passages and 3,485,037 embeddings. From #925,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "38it [36:17, 57.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:08:51] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:09:41] [0] \t\t #> Saving chunk 38: \t 25,000 passages and 3,524,718 embeddings. From #950,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "39it [37:14, 57.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:09:48] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:10:39] [0] \t\t #> Saving chunk 39: \t 25,000 passages and 3,481,003 embeddings. From #975,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "40it [38:12, 57.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:10:46] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:11:37] [0] \t\t #> Saving chunk 40: \t 25,000 passages and 3,505,696 embeddings. From #1,000,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "41it [39:10, 57.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:11:44] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:12:34] [0] \t\t #> Saving chunk 41: \t 25,000 passages and 3,519,486 embeddings. From #1,025,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "42it [40:07, 57.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:12:41] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:13:31] [0] \t\t #> Saving chunk 42: \t 25,000 passages and 3,512,006 embeddings. From #1,050,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "43it [41:04, 57.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:13:38] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:14:28] [0] \t\t #> Saving chunk 43: \t 25,000 passages and 3,515,841 embeddings. From #1,075,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "44it [42:01, 57.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:14:35] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:15:26] [0] \t\t #> Saving chunk 44: \t 25,000 passages and 3,492,342 embeddings. From #1,100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "45it [42:59, 57.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:15:33] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:16:25] [0] \t\t #> Saving chunk 45: \t 25,000 passages and 3,517,306 embeddings. From #1,125,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "46it [43:59, 58.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:16:32] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:17:25] [0] \t\t #> Saving chunk 46: \t 25,000 passages and 3,521,635 embeddings. From #1,150,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "47it [44:58, 58.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:17:32] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:18:24] [0] \t\t #> Saving chunk 47: \t 25,000 passages and 3,499,550 embeddings. From #1,175,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "48it [45:57, 58.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:18:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:19:23] [0] \t\t #> Saving chunk 48: \t 25,000 passages and 3,513,235 embeddings. From #1,200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "49it [46:56, 58.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:19:30] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:20:22] [0] \t\t #> Saving chunk 49: \t 25,000 passages and 3,498,891 embeddings. From #1,225,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "50it [47:55, 58.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:20:29] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:21:20] [0] \t\t #> Saving chunk 50: \t 25,000 passages and 3,513,046 embeddings. From #1,250,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "51it [48:53, 58.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:21:27] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:22:20] [0] \t\t #> Saving chunk 51: \t 25,000 passages and 3,495,427 embeddings. From #1,275,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "52it [49:54, 59.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:22:28] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:23:20] [0] \t\t #> Saving chunk 52: \t 25,000 passages and 3,494,335 embeddings. From #1,300,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "53it [50:53, 59.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:23:27] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:24:19] [0] \t\t #> Saving chunk 53: \t 25,000 passages and 3,526,543 embeddings. From #1,325,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "54it [51:52, 59.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:24:26] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:25:17] [0] \t\t #> Saving chunk 54: \t 25,000 passages and 3,509,530 embeddings. From #1,350,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "55it [52:50, 58.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:25:24] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:26:14] [0] \t\t #> Saving chunk 55: \t 25,000 passages and 3,511,963 embeddings. From #1,375,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "56it [53:47, 58.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:26:21] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:27:11] [0] \t\t #> Saving chunk 56: \t 25,000 passages and 3,518,692 embeddings. From #1,400,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "57it [54:44, 57.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:27:18] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:28:08] [0] \t\t #> Saving chunk 57: \t 25,000 passages and 3,534,418 embeddings. From #1,425,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "58it [55:41, 57.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:28:15] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:29:06] [0] \t\t #> Saving chunk 58: \t 25,000 passages and 3,512,066 embeddings. From #1,450,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "59it [56:39, 57.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:29:13] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:30:05] [0] \t\t #> Saving chunk 59: \t 25,000 passages and 3,486,280 embeddings. From #1,475,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "60it [57:38, 58.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:30:12] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:31:03] [0] \t\t #> Saving chunk 60: \t 25,000 passages and 3,506,326 embeddings. From #1,500,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "61it [58:36, 58.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:31:10] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:32:02] [0] \t\t #> Saving chunk 61: \t 25,000 passages and 3,516,145 embeddings. From #1,525,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "62it [59:35, 58.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:32:09] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:33:01] [0] \t\t #> Saving chunk 62: \t 25,000 passages and 3,510,765 embeddings. From #1,550,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "63it [1:00:34, 58.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:33:08] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:33:59] [0] \t\t #> Saving chunk 63: \t 25,000 passages and 3,482,178 embeddings. From #1,575,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "64it [1:01:31, 58.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:34:05] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:34:56] [0] \t\t #> Saving chunk 64: \t 25,000 passages and 3,510,138 embeddings. From #1,600,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "65it [1:02:29, 58.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:35:03] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:35:54] [0] \t\t #> Saving chunk 65: \t 25,000 passages and 3,511,113 embeddings. From #1,625,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "66it [1:03:27, 57.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:36:01] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:36:52] [0] \t\t #> Saving chunk 66: \t 25,000 passages and 3,545,687 embeddings. From #1,650,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "67it [1:04:25, 57.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:36:59] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:37:50] [0] \t\t #> Saving chunk 67: \t 25,000 passages and 3,489,005 embeddings. From #1,675,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "68it [1:05:23, 58.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:37:57] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:38:47] [0] \t\t #> Saving chunk 68: \t 25,000 passages and 3,492,689 embeddings. From #1,700,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "69it [1:06:20, 57.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:38:54] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:39:44] [0] \t\t #> Saving chunk 69: \t 25,000 passages and 3,502,892 embeddings. From #1,725,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "70it [1:07:17, 57.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:39:51] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:40:41] [0] \t\t #> Saving chunk 70: \t 25,000 passages and 3,491,321 embeddings. From #1,750,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "71it [1:08:14, 57.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:40:48] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:41:38] [0] \t\t #> Saving chunk 71: \t 25,000 passages and 3,513,498 embeddings. From #1,775,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "72it [1:09:11, 57.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:41:45] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:42:34] [0] \t\t #> Saving chunk 72: \t 25,000 passages and 3,517,857 embeddings. From #1,800,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "73it [1:10:07, 57.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:42:41] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:43:31] [0] \t\t #> Saving chunk 73: \t 25,000 passages and 3,501,410 embeddings. From #1,825,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "74it [1:11:04, 57.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:43:38] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:44:28] [0] \t\t #> Saving chunk 74: \t 25,000 passages and 3,511,496 embeddings. From #1,850,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "75it [1:12:01, 56.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:44:35] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:45:27] [0] \t\t #> Saving chunk 75: \t 25,000 passages and 3,501,942 embeddings. From #1,875,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "76it [1:13:00, 57.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:45:34] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:46:27] [0] \t\t #> Saving chunk 76: \t 25,000 passages and 3,512,472 embeddings. From #1,900,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "77it [1:14:00, 58.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:46:34] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:47:25] [0] \t\t #> Saving chunk 77: \t 25,000 passages and 3,484,022 embeddings. From #1,925,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "78it [1:14:57, 58.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:47:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:48:21] [0] \t\t #> Saving chunk 78: \t 25,000 passages and 3,519,119 embeddings. From #1,950,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "79it [1:15:54, 57.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:48:28] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:49:17] [0] \t\t #> Saving chunk 79: \t 25,000 passages and 3,495,361 embeddings. From #1,975,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "80it [1:16:50, 56.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:49:23] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:50:13] [0] \t\t #> Saving chunk 80: \t 25,000 passages and 3,495,482 embeddings. From #2,000,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "81it [1:17:45, 56.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:50:19] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:51:10] [0] \t\t #> Saving chunk 81: \t 25,000 passages and 3,510,742 embeddings. From #2,025,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "82it [1:18:42, 56.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:51:16] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:52:06] [0] \t\t #> Saving chunk 82: \t 25,000 passages and 3,460,015 embeddings. From #2,050,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "83it [1:19:39, 56.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:52:13] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:53:02] [0] \t\t #> Saving chunk 83: \t 25,000 passages and 3,516,439 embeddings. From #2,075,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "84it [1:20:35, 56.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:53:09] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:54:00] [0] \t\t #> Saving chunk 84: \t 25,000 passages and 3,501,295 embeddings. From #2,100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "85it [1:21:33, 56.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:54:07] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:54:57] [0] \t\t #> Saving chunk 85: \t 25,000 passages and 3,503,294 embeddings. From #2,125,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "86it [1:22:30, 56.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:55:04] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:55:53] [0] \t\t #> Saving chunk 86: \t 25,000 passages and 3,511,829 embeddings. From #2,150,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "87it [1:23:26, 56.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:55:59] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:56:49] [0] \t\t #> Saving chunk 87: \t 25,000 passages and 3,499,856 embeddings. From #2,175,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "88it [1:24:22, 56.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:56:56] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:57:45] [0] \t\t #> Saving chunk 88: \t 25,000 passages and 3,514,538 embeddings. From #2,200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "89it [1:25:18, 56.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:57:52] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:58:41] [0] \t\t #> Saving chunk 89: \t 25,000 passages and 3,493,010 embeddings. From #2,225,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "90it [1:26:14, 56.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:58:48] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 03:59:37] [0] \t\t #> Saving chunk 90: \t 25,000 passages and 3,502,251 embeddings. From #2,250,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "91it [1:27:10, 56.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 03:59:44] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 04:00:33] [0] \t\t #> Saving chunk 91: \t 25,000 passages and 3,498,252 embeddings. From #2,275,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "92it [1:28:06, 56.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 04:00:40] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 04:01:29] [0] \t\t #> Saving chunk 92: \t 25,000 passages and 3,509,032 embeddings. From #2,300,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "93it [1:29:02, 56.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 04:01:36] [0] \t\t #> Encoding 25000 passages..\n",
      "[Feb 23, 04:02:25] [0] \t\t #> Saving chunk 93: \t 25,000 passages and 3,501,409 embeddings. From #2,325,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "94it [1:29:58, 55.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 04:02:31] [0] \t\t #> Encoding 10655 passages..\n",
      "[Feb 23, 04:02:52] [0] \t\t #> Saving chunk 94: \t 10,655 passages and 1,495,456 embeddings. From #2,350,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "95it [1:30:21, 46.29s/it]\r",
      "95it [1:30:21, 57.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 04:02:55] [0] \t\t #> Checking all files were saved...\n",
      "[Feb 23, 04:02:55] [0] \t\t Found all files!\n",
      "[Feb 23, 04:02:55] [0] \t\t #> Building IVF...\n",
      "[Feb 23, 04:02:56] [0] \t\t #> Loading codes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 95/95 [00:00<00:00, 241.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 04:02:56] [0] \t\t Sorting codes...\n",
      "[Feb 23, 04:03:18] [0] \t\t Getting unique codes...\n",
      "[Feb 23, 04:03:19] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 23, 04:03:19] #> Building the emb2pid mapping..\n",
      "[Feb 23, 04:03:24] len(emb2pid) = 331048045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 262144/262144 [00:36<00:00, 7259.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 04:04:02] #> Saved optimized IVF to /home/adqaicp/documents/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits/ivf.pid.pt\n",
      "[Feb 23, 04:04:03] [0] \t\t #> Saving the indexing metadata to /home/adqaicp/documents/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use.\n",
    "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits)\n",
    "\n",
    "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "    indexer.index(name=index_name, collection=collection, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "/home/adqaicp/documents/ColBERT/docs/experiments/default/indexes/lifestyle.dev.2bits",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m indexer \u001b[38;5;241m=\u001b[39m Indexer(checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/documents/ColBERT/docs/../colbert/indexer.py:67\u001b[0m, in \u001b[0;36mIndexer.index\u001b[0;34m(self, name, collection, overwrite)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mindex_path_\n\u001b[1;32m     65\u001b[0m index_does_not_exist \u001b[38;5;241m=\u001b[39m (\u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mindex_path_))\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (overwrite \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreuse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m index_does_not_exist, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mindex_path_\n\u001b[1;32m     68\u001b[0m create_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mindex_path_)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overwrite \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: /home/adqaicp/documents/ColBERT/docs/experiments/default/indexes/lifestyle.dev.2bits"
     ]
    }
   ],
   "source": [
    "indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "indexer.index(name=index_name, collection=collection, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adqaicp/documents/ColBERT/docs/experiments/default/indexes/lifestyle.dev.2bits'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.get_index() # You can get the absolute path of the index, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Having built the index and prepared our `searcher`, we can search for individual query strings.\n",
    "\n",
    "We can use the `queries` set we loaded earlier — or you can supply your own questions. Feel free to get creative! But keep in mind this set of ~300k lifestyle passages can only answer a small, focused set of questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# sys.path.insert(0, '../')\n",
    "\n",
    "# from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "# from colbert.data import Queries, Collection\n",
    "# from colbert import Indexer, Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 04:40:08] #> Loading collection...\n",
      "0M 1M 2M \n",
      "[Feb 23, 04:40:16] #> Loading codec...\n",
      "[Feb 23, 04:40:16] #> Loading IVF...\n",
      "[Feb 23, 04:40:16] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:00<00:00, 1821.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 23, 04:40:16] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:03<00:00, 29.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# To create the searcher using its relative name (i.e., not a full path), set\n",
    "# experiment=value_used_for_indexing in the RunConfig.\n",
    "with Run().context(RunConfig(experiment='notebook')):\n",
    "    searcher = Searcher(index=index_name)\n",
    "\n",
    "\n",
    "# If you want to customize the search latency--quality tradeoff, you can also supply a\n",
    "# config=ColBERTConfig(ncells=.., centroid_score_threshold=.., ndocs=..) argument.\n",
    "# The default settings with k <= 10 (1, 0.5, 256) gives the fastest search,\n",
    "# but you can gain more extensive search by setting larger values of k or\n",
    "# manually specifying more conservative ColBERTConfig settings (e.g. (4, 0.4, 4096))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Chinese cooking recepit\n",
      "\t [1] \t\t 18.6 \t\t Chinese Cooking. Cookbook.\n",
      "\t [2] \t\t 18.6 \t\t Chinese Cooking. .\n",
      "\t [3] \t\t 18.5 \t\t Dim Sum: Dumplings, Parcels and Other Delectable Chinese Snacks in 25 Authentic Recipes. Dim sum is a traditional style of eating, where bite-sized tidbits are served for shared dining. This book makes authentic Chinese dim sum accessible to the home cook. It opens with a practical introduction to the cuisine, with essential information on ingredients and equipment. More than 25 recipes follow, with a diverse selection of dishes from all over China..\n",
      "\t [4] \t\t 18.5 \t\t The Everything Chinese Cookbook: From Wonton Soup to Sweet and Sour Chicken-300 Succelent Recipes from the Far East. Featuring hundreds of recipes, such as Snow Pea Stir-fry, Hot Chicken Salad, General Tso's Chicken, and Traditional Mu Shu Pork, The Everything Chinese Cookbookmakes preparing authentic Chinese dishes fun and easy! From basic Chinese flavors and dipping sauces, such as Quick and Easy Sweet-and-Sour Sauce, to Chinese cooking methods and meals, including Stir-fried Orange Beef, The Everything Chinese Cookbookoffers a diverse set of recipes perfect for both vegetarians and meat-eaters. Featuring delicious recipes for: Appetizers, such as Crab Rangoon Soups, such as Wonton Soup Vegetable dishes, such as Stir-fried Baby Bok Choy Beef dishes, such as Mongolian Beef with Rice Noodles Pork dishes, such as Sweet and Sour Spareribs Mouthwatering fiery dishes, such as Spicy Chicken with Cashews Desserts, such as Sweet Baked Pineapple and Banana The Everything Chinese Cookbookwill have you serving up tasty Chinese cuisine to tempt anyone!.\n",
      "\t [5] \t\t 18.4 \t\t Classic Chinese Cuisine. Repeatedly singled out as one of the all-time cookbook bibles, Classic Chinese Cuisine is a thorough introduction to the basics of Chinese cooking, covering all the essential techniques, ingredients, and cooking utensils, with more than 225 recipes, step-by-step illustrations, and full-color photographs. Nina Simonds has an unerring eye for the most approachable and delicious dishes in the Chinese repertoire, from Crispy-Skin Duck and Hundred-Corner Shrimp Balls to Lemon Chicken Wings and Dry-Cooked String Beans..\n",
      "\t [6] \t\t 18.2 \t\t \"Everyday Chinese Cooking: Quick and Delicious Recipes from the Leeann Chin Restaurants. \"\"There are too many exotic ingredients.\"\" . . . \"\"What about all that preparation?\"\" . . . \"\"I don't want to buy special equipment.\"\" . . . Acclaimed restaurateur Leeann Chin and her daughter Katie have heard all the excuses before, and in response they present their collection of delicious, simple recipes that will make any cook feel like a gourmet Chinese chef. Everyday Chinese Cookingproves that the very best Chinese cooking can be achieved in a real home kitchen, by real people, on real schedules. As a young, time-strapped mother cooking for a family of eight on a limited budget -- and in her new home of Minnesota, half a world away from where she was raised -- Leeann Chin developed recipes that worked for her new lifestyle, without access to all the ingredients of her homeland and within the constraints of a very busy life. The results speak for themselves: quick, flavorful, accessible but authentic Chinese dishes that could make you consider opening up your own take-out restaurant. More than 150 recipes encompass appetizers, soups, poultry, beef, pork, seafood, vegetables, noodles, rice, and desserts. Introductions to each recipe provide completely usable information, such as ingredient substitutions, make-ahead tips, serving suggestions, and other ideas for real-life cooking and eating. Everyday Chinese Cookingis more than quick and easy food; it's also naturally healthful. Best of all, once you get a few recipes under your belt (in every sense of the word) you'll realize that Chinese cooking is truly one of the most convenient ways to get dinner on the table with the least amount of stress. Leeann Chin's incredibly successful restaurants have been voted \"\"Best Chinese Food\"\" by Minneapolis & St. Paul Magazine in the Minneapolis area for more than a dozen consecutive years. With the help of her daughter Katie, Leeann proves that Chinese food can -- and should -- be an everyday option for home cooks of all experience levels, everywhere.. \"\n",
      "\t [7] \t\t 18.2 \t\t Chinese Regional Cooking. .\n",
      "\t [8] \t\t 18.1 \t\t Chinese Food Made Easy. Ching-He Huang is one of the brightest stars in modern Chinese cooking in the UK. Each week in her new BBC2 series she re-invents the nation's favourite Chinese dishes, modernising them with fresh, easy to buy ingredients, and offering simple practical tips and techniques. These are brought together in this beautiful book to accompany the series.Drawing on the experiences of top chefs, her family and friends, growers and producers and celebrity enthusiasts Ching sets out to discover the best Chinese cooking in the UK today, introducing easy-to-make Chinese food to sometimes resistant Brits, and painting a picture of modern Anglo-Chinese life in the UK as she goes.Ching's Chinese Kitchen begins with some of the most familiar dishes from a Chinese takeaway menu - Sweet & Sour Prawns, Chicken with Cashew Nuts, Chop Suey and Cantonese Vegetable Stir Fry, each with Ching's special and imaginative twist. Later we explore spicy Szechuan food: Noodles, Dumplings and Dim sum; Seafood; Fast Food ; Desserts and finally Celebratory Food, where Ching presents a complete banquet of dishes to celebrate the Chinese New Year.Ching's knowledge, charm and enthusiasm shine through as she shares the 'basic principles' of Chinese cooking including some of the simple techniques and tips taught by her Grandparents for tasty results. Using ingredients from high-street supermarkets and some imaginative suggestions for alternative ingredients, these classic Chinese dishes are updated, fresh and healthily prepared so that anyone can make and enjoy them..\n",
      "\t [9] \t\t 18.1 \t\t \"Authentic Chinese Cuisine: For the Contemporary Kitchen. This book stands out among the many Chinese vegetarian cookbooks with its innovative recipes for \"\"mock meat\"\" dishes, just like those you enjoy in Chinese restaurants. Sections include: regional cooking in China; planning a Chinese meal; shopping for essential Chinese ingredients; meat substitutes for the right taste and texture. These recipes are as authentic as possible without calling for extremely exotic ingredients or special Chinese equipment to prepare them. These recipes also show that Chinese vegetarian cuisine can provide variation and culinary delight along with nutritional excellence.. \"\n",
      "\t [10] \t\t 18.0 \t\t Simple Chinese Cooking. .\n"
     ]
    }
   ],
   "source": [
    "# query = queries[37]   # or supply your own query\n",
    "# query = \"what is applewatch?\"\n",
    "# query = queries[31]\n",
    "# query = \"hello\"\n",
    "# query = \"learn SQL\"\n",
    "# query = \"prepare a luggage for travel\"\n",
    "query = \"Chinese cooking recepit\"\n",
    "# query = queries[3]\n",
    "\n",
    "\n",
    "print(f\"#> {query}\")\n",
    "\n",
    "# Find the top-3 passages for this query 【query数量】\n",
    "results = searcher.search(query, k=10)\n",
    "# results = searcher.search(query, k=10)\n",
    "\n",
    "# Print out the top-k retrieved passages\n",
    "for passage_id, passage_rank, passage_score in zip(*results):\n",
    "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Search\n",
    "\n",
    "In many applications, you have a large batch of queries and you need to maximize the overall throughput. For that, you can use the `searcher.search_all(queries, k)` method, which returns a `Ranking` object that organizes the results across all queries.\n",
    "\n",
    "(Batching provides many opportunities for higher-throughput search, though we have not implemented most of those optimizations for compressed indexes yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 417/417 [00:01<00:00, 224.98it/s]\n"
     ]
    }
   ],
   "source": [
    "rankings = searcher.search_all(queries, k=5).todict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24367, 1, 16.078125),\n",
       " (35359, 2, 15.8046875),\n",
       " (131545, 3, 15.7421875),\n",
       " (3789, 4, 15.7421875),\n",
       " (25089, 5, 15.6640625)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings[30]  # For query 30, a list of (passage_id, rank, score) for the top-k passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a99ac6d2deb03d0b7ced3594556c328848678d7cea021ae1b9990e15d3ad5c49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
